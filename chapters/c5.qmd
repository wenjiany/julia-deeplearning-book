# Chapter 5: Pet breeds

```{julia}
#| echo: false
#| output: false

using Pkg;
Pkg.activate(".");

# Packages
using DataFrames
using Flux
using Images
using Measures
using MLDatasets
using MLUtils
using OneHotArrays
using Plots
using Statistics
using CodecZlib
using Downloads
using Random

import UnicodePlots
import Tar

# File paths:
www_path = "www"
data_path = "data"

```

## Image Classification

Download data (script from tjburch)

```{julia}

function extract_tar(filename::String, output_folder::String)
    println("Extracting...")
    open(filename) do file
        stream = GzipDecompressorStream(file)
        Tar.extract(stream, output_folder)
    end
end

output_folder = joinpath(data_path, "pets")

# Ensure the output folder exists
if !isdir(output_folder)
    mkdir(output_folder)
end

tar_filename = joinpath(data_path, "downloaded_file.tar")

url = "https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz"

# Download the tarball if it doesn't exist
if !isfile(tar_filename)
    Downloads.download(url, tar_filename)
else
    println("$tar_filename already exists. Skipping download.")
end

# Extract tarball

img_path = joinpath(output_folder, "oxford-iiit-pet", "images")

files = isdir(img_path) ? readdir(img_path) : []

if length(files) == 0
    extract_tar(tar_filename, output_folder)
else
    println("Images already extracted. Skipping extraction.")
end

```

```{julia}
#| output: false
#| eval: false

files = readdir(img_path)

```


```{julia}
fname = readdir(img_path)[1]

image = load(joinpath(img_path, fname))


```

```{julia}

findall(str -> occursin(r"(.+)_\d+.jpg$", str), files)  ## will find the location indices

filter(str -> occursin(r"(.+)_\d+.jpg$", str), files) ## will return the matching values

```

```{julia}

# Load all images
function load_images(folder::String)
    images = []
    for file in readdir(folder)
        if occursin(".jpg", file)
            path = joinpath(folder, file)
            push!(images, load(path))
        end
    end
    return images
end

images = load_images(img_path)

println("Loaded $(length(images)) images.")

```

Grizzly bear example

```{julia}

image = load(Downloads.download("https://github.com/fastai/fastbook/blob/master/images/grizzly.jpg?raw=true"))

```


```{julia}
typeof(image)

channelview(image)

size(channelview(image))

red.(image) * 255
green.(image) * 255
blue.(image) * 255

```

Image augmentation

```{julia}

# x1 = x1.affine_coord(sz=224)
# x1 = x1.rotate(draw=30, p=1.)
# x1 = x1.zoom(draw=1.2, p=1.)
# x1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.)

using Augmentor

p1 = Resize(460, 460)

p2 = Rotate(-30)  |>
     Zoom(1.8) |>
     Resize(224, 224)

img_new = augment(image, p1 |> p2)

```

```{julia}
#| eval: false

## implimnetation of prospective warp
## not working yet...
using StaticArrays, CoordinateTransformations

M = @SMatrix [1 0 0; 0 1 0; -1/1000 0 1] 

tform = PerspectiveMap() ∘ inv(LinearMap(M))

push1(x) = push(x, 1)

tform2 = PerspectiveMap() ∘ inv(LinearMap(M)) ∘ push1

tform2(@SVector([1,1]))

imgw = warp(image, tform2, Images.indices_spatial(image)); 
hcat(image, imgw)

```
Define DataBlock type

```{julia}

### python
## pets = DataBlock(blocks = (ImageBlock, CategoryBlock),
##                  get_items=get_image_files, 
##                  splitter=RandomSplitter(seed=42),
##                  get_y=using_attr(RegexLabeller(r'(.+)_\d+.jpg$'), 'name'),
##                  item_tfms=Resize(460),
##                  batch_tfms=aug_transforms(size=224, min_scale=0.75))


## create an alias for Matrix{RGB{N0f8}} as ImgMatrix
const ImgMatrix = typeof(image)

struct TransformBlock{T}
    item::T
end

imgBlock = TransformBlock{ImgMatrix}(image)

struct DataBlock{T,U}
    inputBlocks::Vector{TransformBlock{T}}
    outputBlocks::Vector{TransformBlock{U}}
end


function get_image_files(path::String)
    files = readdir(path)
    return filter(str -> occursin(r"(.+)_\d+.jpg$", str), files)
end

image_files = get_image_files(img_path)

function regexLabeller(r::Regex)
    function _inner(o::String)
        return String(match(r, o)[1])
    end
    return _inner
end

breedLabeller = regexLabeller(r"(.+)_\d+.jpg$")

image_names  = breedLabeller.(image_files)
## image_names = [match(r"^(.+)_\d+.jpg$", x)[1] for x in image_files]

pets = DataBlock{ImgMatrix, String}(
    TransformBlock{ImgMatrix}.(images), 
    TransformBlock{String}.(image_names)
)

#findall(pets.inputBlocks)
findall(img -> size(channelview(img.item))[1] == 4, pets.inputBlocks)  ## will find the location indices

xx = map(img -> size(channelview(img.item))[1], pets.inputBlocks)  ## will find the location indices

using StatsBase
StatsBase.countmap(xx)

```

```{julia}

## return two DataBlocks, training and validation
## todo
function randmSplitter(db::DataBlock{T, U}, seed::Int64=42) where {T, U}

    x = db.inputBlocks
    y = db.outputBlocks

    Random.seed!(seed)
    
    function _split()
        idxs = shuffle(1:length(y))
        cut = round(Int, 0.8 * length(idxs))

        trainidxs, valididxs = idxs[1:cut], idxs[cut+1:end]
        
        train_x, valid_x = x[trainidxs], x[valididxs]
        train_y, valid_y = y[trainidxs], y[valididxs]
        
        return DataBlock(train_x, train_y), DataBlock(valid_x, valid_y)
    end
    return _split
end

train_db, valid_db = randmSplitter(pets, 42)()

get_items(bl::TransformBlock{ImgMatrix}) = bl.item

get_y(bl::TransformBlock{String}) = bl.item
get_y(pets.outputBlocks[1])

## run on CPU and then move to GPU
item_tfms(bl::TransformBlock{ImgMatrix}) = TransformBlock{ImgMatrix}(augment(bl.item, Resize(460)))
item_tfms(pets.inputBlocks[1])

## run on GPU
batch_tfms = function(bls::Vector{TransformBlock{ImgMatrix}}, tfms::Augmentor.ImageOperation)
    [TransformBlock(augment(bl.item, tfms)) for bl in bls]
end

batch_tfms(pets.inputBlocks[1:3], Resize(224, 224))


Base.length(db::DataBlock) = length(db.inputBlocks)

length(train_db)


```

create a DataLoader using DataBlock as input, 
and return a DataLoader of data in the format that can be fed into Metalhead models

```{julia}

function image2array(bl::TransformBlock{ImgMatrix})
    return permutedims(channelview(bl.item), [2, 3, 1])
end

function onehot(bl::TransformBlock{String}, labels::Vector{String}=String[])
    return onehotbatch([bl.item], labels)
end

function DataBlockLoader(bl::DataBlock; batchsize=256, shuffle=true, tfms::Augmentor.ImageOperation=Resize(224))

    distinct_labels = unique(get_y.(bl.outputBlocks))

    ## do a batch transform on the inputBlocks
    ## convert inputBlocks to a vector of images of size (224, 224, 3, blocksize)
    input = image2array.(batch_tfms(bl.inputBlocks, tfms))

    ## convert outputBlocks to a vector of labels of size (blocksize,)
    output = [onehot(x, distinct_labels) for x in pets.outputBlocks]

    return DataLoader(collect(zip(input, output)), batchsize=batchsize, shuffle=shuffle)
end

## dset = [(train_x[i, :], train_y[i]) for i in range(1, size(train_x)[1])]

dset = [(train_db.inputBlocks[i], train_db.outputBlocks[i]) for i in range(1, length(train_db))]
dl = DataLoader(dset, batchsize=256, shuffle=true)

train_set = DataBlockLoader(train_db; batchsize=256, shuffle=true, tfms=Resize(224, 224))

```
Try Metalhead Guitar example

```{julia}

using Metalhead
using DataAugmentation

X = []
Y = []

img = Images.load(Downloads.download("https://cdn.pixabay.com/photo/2015/05/07/11/02/guitar-756326_960_720.jpg"));

DATA_MEAN = (0.485, 0.456, 0.406)
DATA_STD = (0.229, 0.224, 0.225)

augmentations = CenterCrop((224, 224)) |>  ImageToTensor() |>  Normalize(DATA_MEAN, DATA_STD)

data = apply(augmentations, Image(img)) |> itemdata
size(data)

# ImageNet labels
labels = readlines(Downloads.download("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"))

model = ResNet(18; pretrain = true);

## output imagenet classes, need to change to pets...
size(Flux.unsqueeze(data, 4))
model(Flux.unsqueeze(data, 4))

println(onecold(model(Flux.unsqueeze(data, 4)), labels))

data2 = stack([data, data], dims=4);
size(data2)
model(data2)
println(onecold(model(data2), labels))


model = Chain(
    Metalhead.ResNet(18; pretrain=true).layers,  # Use all layers of ResNet and use pretrained weights
)

println(onecold(model(data2), labels))

model = Chain(
    Metalhead.ResNet(18).layers,  # Use all layers of ResNet and use pretrained weights
)

println(onecold(model(data2), labels))

img = images[10]

println(onecold(model(data2), labels))

```

Adapt the above to use the pets data
Simplify the above code in order to use GPU, and not use custom types


```{julia}

pet_labels = sort(unique(image_names))
n_breeds = length(pet_labels)

## how to move augmentation to GPU?
## X = [channelview(augment(img, p1)) for img in images] |> gpu
X = [channelview(augment(dl.item, p1 |> p2)) for dl in train_db.inputBlocks] |> gpu
X = [permutedims(img, (2, 3, 1)) for img in X]


Y = onehotbatch(get_y.(train_db.outputBlocks), pet_labels) |> gpu

## Y = [onehotbatch([yb.item], pet_labels) for yb in train_db.outputBlocks] |> gpu

model = Chain(
    Metalhead.ResNet(18; pretrain=true).layers[1],  # Use all layers of ResNet except the last dense layer
    Chain(
      AdaptiveMeanPool((1, 1)),
        MLUtils.flatten,
        Dense(512 => n_breeds),
    ),
    # Dense(512, 37),  # Dense layer for number of breeds
    softmax
) |> gpu

loss(x, y) = Flux.crossentropy(model(x), y) |> gpu
opt = ADAM(0.001)

model(Flux.unsqueeze(X[10], dims=4))

X = stack(X);

model(X[:, :, :, 1:10])
loss(X[:,:,:,1:10], Y[:, 1:10])

accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))
println("Accuracy: ", accuracy(X[:,:,:,1:20], Y[:, 1:20]))

## try a minibatch


## evalcb = () -> @show(loss(X, Y))

# Assuming model structure like the one in the previous answer
trainable_params = Flux.params(model[2])  # Only parameters of the last Dense layer

onecold(model(X[:, :, :, 10:11]), pet_labels)
image_names[10:11]

accuracy(X[:, :, :, 10:11], Y[:, 10:11])

accuracy(X[:, :, :, 1:100], Y[:, 1:100])

data =[(X[:, :, :, 1:11], Y[:, 1:11])]
Flux.train!(loss, trainable_params, data, opt)

onecold(model(X[:, :, :, 10:11]), pet_labels)
image_names[10:11]

pets_dl = DataLoader(1:size(Y)[2], batchsize=10, shuffle=true)
data_idx = first(pets_dl)

data =[(X[:, :, :, data_idx], Y[:, data_idx])];
Flux.train!(loss, trainable_params, data, opt)

epochs = 10

for epoch = 1:epochs
    println("Epoch: $epoch")
##     Flux.train!(loss, Flux.params(model), dataset, opt, cb = throttle(evalcb, 10))

    for (i, data_idx) in enumerate(pets_dl)
        println(i)
        data =[(X[:, :, :, data_idx], Y[:, data_idx])];
        Flux.train!(loss, trainable_params, data, opt)
    end
end

```



