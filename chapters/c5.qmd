# Chapter 5: Pet breeds

```{julia}
#| echo: false
#| output: false

using Pkg;
Pkg.activate(".");

# Packages
using DataFrames
using Flux
using Images
using ImageView
using Measures
using MLDatasets
using MLUtils
using OneHotArrays
using Plots
using Statistics
using CodecZlib
using Downloads
using Random
using Loess

import UnicodePlots
import Tar

# File paths:
www_path = "www"
data_path = "data"

```

## Image Classification

Download data (script from tjburch)

```{julia}

function extract_tar(filename::String, output_folder::String)
    println("Extracting...")
    open(filename) do file
        stream = GzipDecompressorStream(file)
        Tar.extract(stream, output_folder)
    end
end

output_folder = joinpath(data_path, "pets")

# Ensure the output folder exists
if !isdir(output_folder)
    mkdir(output_folder)
end

tar_filename = joinpath(data_path, "downloaded_file.tar")

url = "https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz"

# Download the tarball if it doesn't exist
if !isfile(tar_filename)
    Downloads.download(url, tar_filename)
else
    println("$tar_filename already exists. Skipping download.")
end

# Extract tarball

img_path = joinpath(output_folder, "oxford-iiit-pet", "images")

files = isdir(img_path) ? readdir(img_path) : []

if length(files) == 0
    extract_tar(tar_filename, output_folder)
else
    println("Images already extracted. Skipping extraction.")
end

```


```{julia}

findall(fname -> occursin(r"(.+)_\d+.jpg$", fname), files)  ## will find the location indices

filter(fname -> occursin(r"(.+)_\d+.jpg$", fname), files) ## will return the matching values

function get_image_files(path::String)
    files = readdir(path)
    return filter(fname -> occursin(r"(.+)_\d+.jpg$", fname), files)
end

image_files = get_image_files(img_path)

function regexLabeller(r::Regex)
    function _inner(o::String)
        return String(match(r, o)[1])
    end
    return _inner
end

breedLabeller = regexLabeller(r"(.+)_\d+.jpg$")

image_names  = breedLabeller.(image_files)
## image_names = [match(r"^(.+)_\d+.jpg$", x)[1] for x in image_files]

image = load(joinpath(img_path, image_files[1]))

ImageView.imshow(image)

```

According to https://www.juliapackages.com/p/fixedpointnumbers, "N0f8(aliased to Normed{UInt8,8}) is represented internally by a UInt8, and makes 0x00 equivalent to 0.0 and 0xff to 1.0."

```{julia}

# Load all images
function load_images(folder::String)
    ## images = Vector{Array{RGB, 2}}()  # significantly higher memory allocation
    ## images = Vector{Array{RGB{N0f8}, 2}}()  # similiar memory allocation as []
    images = []
    for file in readdir(folder)
        if occursin(".jpg", file)
            try
                path = joinpath(folder, file)
                push!(images, load(path))
            catch
                println("$file failed to load")
            end
        end
    end
    return images
end

images = load_images(img_path);

println("Loaded $(length(images)) images.")

```

Grizzly bear example for augmentation

```{julia}

image = load(Downloads.download("https://github.com/fastai/fastbook/blob/master/images/grizzly.jpg?raw=true"))

```


```{julia}
typeof(image)

channelview(image)

size(channelview(image))

red.(image) * 255
green.(image) * 255
blue.(image) * 255

```

Image augmentation

Based on the idea of fastai book, images are first resized to a larger 460x460 image, and moved to GPU. Then on GPU, a batched augmentation including rotation, warp and resizeing to 224x224 was performed on a minibatch of images. I am not clear how to implement this strategy in Julia at this moment.

```{julia}

### python, fastai
# x1 = x1.affine_coord(sz=224)
# x1 = x1.rotate(draw=30, p=1.)
# x1 = x1.zoom(draw=1.2, p=1.)
# x1 = x1.warp(draw_x=-0.2, draw_y=0.2, p=1.)

using Augmentor

p1 = Resize(460, 460) ## done on CPU, and should be moved to GPU after this transformation

## p2 should be performed on GPU for the minibatch
p2 = Rotate(-30)  |>
     Zoom(1.8) |>
     Resize(224, 224)

img_new = augment(image, p1 |> p2)

imshow(img_new)

```
It is not clear how prospective warp is implemented in Julia.

```{julia}
#| eval: false

## implimnetation of prospective warp
## not working yet...
### using StaticArrays, CoordinateTransformations
### 
### M = @SMatrix [1 0 0; 0 1 0; -1/1000 0 1] 
### 
### tform = PerspectiveMap() ∘ inv(LinearMap(M))
### 
### push1(x) = push(x, 1)
### 
### tform2 = PerspectiveMap() ∘ inv(LinearMap(M)) ∘ push1
### 
### tform2(@SVector([1,1]))
### 
### imgw = warp(image, tform2, Images.indices_spatial(image)); 
### hcat(image, imgw)
### 
```

Define DataBlock type


```{julia}

### python
## pets = DataBlock(blocks = (ImageBlock, CategoryBlock),
##                  get_items=get_image_files, 
##                  splitter=RandomSplitter(seed=42),
##                  get_y=using_attr(RegexLabeller(r'(.+)_\d+.jpg$'), 'name'),
##                  item_tfms=Resize(460),
##                  batch_tfms=aug_transforms(size=224, min_scale=0.75))

## create an alias for Matrix{RGB{N0f8}} as ImgMatrix
const ImgMatrix = typeof(image)

struct TransformBlock{T}
    item::T
end

imgBlock = TransformBlock{ImgMatrix}(image)
imshow(imgBlock.item)

Base.summarysize(image)
Base.summarysize(imgBlock.item)
### they are referening to the same memory
imgBlock.item===image

struct DataBlock{T,U}
    inputBlocks::Vector{TransformBlock{T}}
    outputBlocks::Vector{TransformBlock{U}}
end

pets = DataBlock{ImgMatrix, String}(
    TransformBlock{ImgMatrix}.(images), 
    TransformBlock{String}.(image_names)
)

### they are refering to the same thing
pets.inputBlocks[1].item === images[1]

#findall(pets.inputBlocks)
findall(img -> size(channelview(img.item))[1] == 4, pets.inputBlocks)  ## will find the location indices

image_channels = map(img -> size(channelview(img.item))[1], pets.inputBlocks)  ## will find the location indices

using StatsBase
StatsBase.countmap(image_channels)

```

check memory usage, we have two "copies" of the data: 
images, pets::DataBlock.

```{julia}
for name in names(Main)
    obj = getfield(Main, name)    
    println("$name: ", Base.summarysize(obj), " bytes")
end
```

But they are not really stored as two different copies.

```{julia}
### they are refering to the same thing
pets.inputBlocks[1].item === images[1]

```

GPT4 says, "In Julia, variables are just references (bindings) to values, whether those values are stored in mutable or immutable structures."

```{julia}

mat = Matrix{RGB{N0f8}}(undef, 2, 2)
mat[1, 1] = RGB{N0f8}(1.0, 0, 0) # Assigning a red color to the element at position (1,1)
imshow(mat)

struct T
    m::Matrix{RGB{N0f8}}
end

mat2 = mat

t = T(mat2)
t.m[1, 1] = RGB{N0f8}(0, 1.0, 0) # Assigning a green color to the element at position (1,1) of mat2
imshow(mat)

```

```{julia}

## return two DataBlocks, training and validation
## todo
function randmSplitter(db::DataBlock{T, U}, seed::Int64=42) where {T, U}

    x = db.inputBlocks
    y = db.outputBlocks

    Random.seed!(seed)
    
    function _split()
        idxs = shuffle(1:length(y))
        cut = round(Int, 0.8 * length(idxs))

        trainidxs, valididxs = idxs[1:cut], idxs[cut+1:end]
        
        train_x, valid_x = x[trainidxs], x[valididxs]
        train_y, valid_y = y[trainidxs], y[valididxs]
        
        return DataBlock(train_x, train_y), DataBlock(valid_x, valid_y)
    end
    return _split
end

train_db, valid_db = randmSplitter(pets, 42)()

get_items(bl::TransformBlock{ImgMatrix}) = bl.item

get_y(bl::TransformBlock{String}) = bl.item
get_y(pets.outputBlocks[1])

## run on CPU and then move to GPU
item_tfms(bl::TransformBlock{ImgMatrix}) = TransformBlock{ImgMatrix}(augment(bl.item, Resize(460)))
item_tfms(pets.inputBlocks[1])

## run on GPU
batch_tfms = function(bls::Vector{TransformBlock{ImgMatrix}}, tfms::Augmentor.ImageOperation)
    [TransformBlock(augment(bl.item, tfms)) for bl in bls]
end


batch_tfms = function(bls::Vector{TransformBlock{ImgMatrix}}, tfms::Augmentor.ImmutablePipeline)
    [TransformBlock(augment(bl.item, tfms)) for bl in bls]
end

batch_tfms(pets.inputBlocks[1:3], Resize(224, 224))
batch_tfms(pets.inputBlocks[1:3], p1 |> p2)

Base.length(db::DataBlock) = length(db.inputBlocks)

length(train_db)


```

create a DataLoader using DataBlock as input, 
and return a DataLoader of data in the format that can be fed into Metalhead models

```{julia}

function image2array(bl::TransformBlock{ImgMatrix})
    return permutedims(channelview(bl.item), [2, 3, 1])
end

function onehot(bl::TransformBlock{String}, labels::Vector{String}=String[])
    return onehotbatch([bl.item], labels)
end

function DataLoader(bl::DataBlock; batchsize=10, shuffle=true, tfms::Augmentor.Pipeline=Augmentor.Pipeline(Resize(224, 224)))

    distinct_labels = unique(get_y.(bl.outputBlocks))

    ## do a batch transform on the inputBlocks
    ## convert inputBlocks to a vector of images of size (224, 224, 3, blocksize)
    input = image2array.(batch_tfms(bl.inputBlocks, tfms))

    ## convert outputBlocks to a vector of labels of size (blocksize,)
    output = [onehot(x, distinct_labels) for x in pets.outputBlocks]

    return Flux.DataLoader(collect(zip(input, output)), batchsize=batchsize, shuffle=shuffle)
end

dset = [(train_db.inputBlocks[i], train_db.outputBlocks[i]) for i in range(1, length(train_db))]
dl = DataLoader(dset, batchsize=10, shuffle=true)

train_set = DataLoader(train_db; batchsize=10, shuffle=true, tfms=Augmentor.Pipeline(Resize(224, 224)))

```
Try Metalhead Guitar example

```{julia}

using Metalhead
using DataAugmentation

X = []
Y = []

img = Images.load(Downloads.download("https://cdn.pixabay.com/photo/2015/05/07/11/02/guitar-756326_960_720.jpg"));

DATA_MEAN = (0.485, 0.456, 0.406)
DATA_STD = (0.229, 0.224, 0.225)

augmentations = CenterCrop((224, 224)) |>  ImageToTensor() |>  Normalize(DATA_MEAN, DATA_STD)

data = apply(augmentations, Image(img)) |> itemdata
size(data)

# ImageNet labels
labels = readlines(Downloads.download("https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"))

model = ResNet(18; pretrain = true);

## output imagenet classes, need to change to pets...
size(Flux.unsqueeze(data, 4))
model(Flux.unsqueeze(data, 4))

println(onecold(model(Flux.unsqueeze(data, 4)), labels))

data2 = stack([data, data], dims=4);
size(data2)
model(data2)
println(onecold(model(data2), labels))


model = Chain(
    Metalhead.ResNet(18; pretrain=true).layers,  # Use all layers of ResNet and use pretrained weights
)

println(onecold(model(data2), labels))

model = Chain(
    Metalhead.ResNet(18).layers,  # Use all layers of ResNet and use pretrained weights
)

println(onecold(model(data2), labels))

img = images[10]

println(onecold(model(data2), labels))

```

Adapt the above to use the pets data
Simplify the above code in order to use GPU, and not use custom types


```{julia}

pet_labels = sort(unique(image_names))
n_breeds = length(pet_labels)

## prepare data for Flux

function prepare_flux_data(db::DataBlock, labels, tfms::Augmentor.ImmutablePipeline=Augmentor.Pipeline(Resize(224, 224)))

    X = batch_tfms(db.inputBlocks, tfms)
    X = [channelview(bl.item) for bl in X] 
    X = stack([permutedims(img, (2, 3, 1)) for img in X])
    X = Array{Float32}(X)

    Y = onehotbatch(get_y.(db.outputBlocks), labels)
    Y = Matrix{Int8}(Y)

    return X, Y
end

train_X, train_Y = prepare_flux_data(train_db, pet_labels, p1 |> p2) |> gpu
valid_X, valid_Y = prepare_flux_data(valid_db, pet_labels) |> gpu

function DataLoader(bl::DataBlock, labels; batchsize=10, shuffle=true, tfms::Augmentor.ImmutablePipeline=Augmentor.Pipeline(Resize(224, 224)))
    X, Y = prepare_flux_data(bl, labels, tfms)
    return Flux.DataLoader((X, Y), batchsize=batchsize, shuffle=shuffle)
end

### will the below work?
## train_dl = DataLoader(train_db, pet_labels; batchsize=10, shuffle=true, tfms=p1 |> p2)

model = Chain(
    Metalhead.ResNet(18; pretrain=true).layers[1],  # Use all layers of ResNet except the last dense layer
    Chain(
      AdaptiveMeanPool((1, 1)),
        MLUtils.flatten,
        Dense(512 => n_breeds),     # Dense(512, 37), Dense layer for number of breeds
    ),
    softmax
) |> gpu

loss(x, y) = Flux.crossentropy(model(x), y) |> gpu

### seems to be more flexible to pass model as a parameter...
loss(m, x, y) = Flux.crossentropy(m(x), y) |> gpu

onecold(model(train_X[:, :, :, 10:11]), pet_labels)
image_names[10:11]

### model refer to the model defined earlier in global scope
accuracy(x, y) = mean(onecold(model(x)) .== onecold(y)) |> gpu

println("Accuracy: ", accuracy(train_X[:,:,:,1:20], train_Y[:, 1:20]))

evalcb = () -> @show(loss(train_X[:,:,:,1:100], train_Y[:, 1:100])) |> gpu

trainable_params = Flux.params(model[2])  # Only parameters of the last Dense layer

opt = ADAM(0.001) |> gpu

pets_dl = Flux.DataLoader((train_X, train_Y), batchsize=10, shuffle=true) |> gpu

epochs = 10

for epoch = 1:epochs
    println("Epoch: $epoch")
##     Flux.train!(loss, Flux.params(model), dataset, opt, cb = throttle(evalcb, 10))

##    Flux.train!(loss, trainable_params, pets_dl, opt, cb = evalcb)
    Flux.train!(loss, trainable_params, pets_dl, opt)
end

accuracy(train_X[:,:,:, 101:200], train_Y[:, 101:200])

accuracy(valid_X, valid_Y)

```

```{julia}

## xx = Flux.glorot_uniform(size(layer_to_reset[3].weight)...) |> gpu
## layer_to_reset[3].weight .= xx

## reset trainable_params to be uniform

# trainable_params[1] .= (Flux.glorot_uniform(size(trainable_params[1])...) |> gpu)
# trainable_params[2] .= (Flux.glorot_uniform(size(trainable_params[2])...) |> gpu)

# accuracy(train_X[:,:,:, 101:200], train_Y[:, 101:200])

# accuracy(valid_X, valid_Y)

## doesn't work
## trainable_params[1] .= (Flux.glorot_uniform(size(trainable_params[1])...))

```

```{julia}

### alternatively to use update

pets_dl = Flux.DataLoader(1:size(train_Y)[2], batchsize=10, shuffle=true) |> gpu
data_idx = first(pets_dl)

for epoch = 1:epochs
    println("Epoch: $epoch")
    
    for (i, data_idx) in enumerate(pets_dl)
        ## print(i)
        data =[(train_X[:, :, :, data_idx], train_Y[:, data_idx])];

       ##  Flux.train!(loss, trainable_params, data, opt)

        x = data[1][1]
        ### batch transform on x here on gpu
        
        y = data[1][2]
        grads = gradient(()->loss(x, y), trainable_params)
        Flux.update!(opt, trainable_params, grads.grads)
    end
end

accuracy(train_X[:,:,:, 101:200], train_Y[:, 101:200])

accuracy(train_X, train_Y)

accuracy(valid_X, valid_Y)

```

Learning rate finder...

```{julia}

lr = 10 .^ (collect(range(-6, stop=1, step=0.2) ))
lr_losses = zeros(length(lr))

pets_dl = Flux.DataLoader(1:size(train_Y)[2], batchsize=10, shuffle=true) |> gpu

for (i, data_idx) in enumerate(pets_dl)
    print(i)
    if (i > length(lr))
        break
    end

    ## reset parameters
    trainable_params[1] .= (Flux.glorot_uniform(size(trainable_params[1])...) |> gpu)
    trainable_params[2] .= (Flux.glorot_uniform(size(trainable_params[2])...) |> gpu)

    opt = ADAM(lr[i]) |> gpu
    data =[(train_X[:, :, :, data_idx], train_Y[:, data_idx])];

    ##  Flux.train!(loss, trainable_params, data, opt)

    x = data[1][1]
    y = data[1][2]
    grads = gradient(()->loss(x, y), trainable_params)
    Flux.update!(opt, trainable_params, grads.grads)

    lr_losses[i] = loss(x, y)
end

## using Loess

xs = log10.(lr)
ys = lr_losses

ml = loess(xs, ys, span=0.5)

us = range(extrema(xs)...; step = 0.1)
vs = predict(ml, us)

us[argmin(vs)]
scatter(xs, ys)
plot!(us, vs, legend=false)

```

Put lr_finder into a funciton

```{julia}

typeof(trainable_params)

## to do methods to reset param

function reset_params!(params::Flux.Zygote.Params, func::Function=Flux.glorot_uniform)
    n_layers = length(trainable_params)
    for i in 1:n_layers
        params[i] .= (func(size(params[i])...) |> gpu)
    end
end |> gpu

function reset_params!(params::Flux.Zygote.Params, reset_to::Flux.Zygote.Params)
    n_layers = length(params)
    for i in 1:n_layers
        params[i] .= reset_to[i]
    end
end |> gpu

orig_params = deepcopy(trainable_params)
trainable_params
reset_params!(trainable_params)
trainable_params
reset_params!(trainable_params, Flux.glorot_uniform)
trainable_params
reset_params!(trainable_params, orig_params)
trainable_params

```

```{julia}

### model is a global variable

lr_range = 10 .^ (collect(range(-6, stop=1, step=0.2) ))

lr_finder = function(params, dl::MLUtils.DataLoader, opt, lr_range; reset_method::Union{Flux.Zygote.Params, Function}=Flux.glorot_uniform)

    lr = collect(lr_range)
    lr_losses = zeros(length(lr))

    for (i, data_idx) in enumerate(dl)
        print(i)
        if (i > length(lr))
            break
        end

        reset_params!(params, reset_method)

        opt = ADAM(lr[i]) |> gpu
        
        data = [first(dl)]

        Flux.train!(loss, params, data, opt)

        lr_losses[i] = loss(data[1][1], data[1][2])
    end

    xs = log10.(lr)
    ys = lr_losses

    ml = loess(xs, ys, span=0.5)

    us = range(extrema(xs)...; step = 0.1)
    vs = predict(ml, us)

    best_lr = us[argmin(vs)]
    
    pp = scatter(log10.(lr), lr_losses)
    plot!(us, vs, legend=false)
    
    return best_lr, (lr=lr, losses=lr_losses), pp
end

pets_dl = Flux.DataLoader((train_X, train_Y), batchsize=10, shuffle=true) |> gpu

best_lr, lr_losses, lr_plot = lr_finder(trainable_params, pets_dl, Adam(0.001), lr_range)

```
	
Fine tune...


```{julia}
## make sure start from a fresh model

model = Chain(
    Metalhead.ResNet(18; pretrain=true).layers[1],  # Use all layers of ResNet except the last dense layer
    Chain(
      AdaptiveMeanPool((1, 1)),
        MLUtils.flatten,
        Dense(512 => n_breeds),
    ),
    softmax
) |> gpu

## no need to redefine loss funciton
# loss(x, y) = Flux.crossentropy(model(x), y) |> gpu

trainable_params = Flux.params(model[2])  # Only parameters of the last Dense layer
freezed_params = Flux.params(model[1])  # Pretrained parameters

pets_dl = Flux.DataLoader((train_X, train_Y), batchsize=10, shuffle=true) |> gpu

opt = ADAM(0.001) |> gpu

epochs=3
for epoch = 1:epochs
    println("Epoch: $epoch")
    Flux.train!(loss, trainable_params, pets_dl, opt)
end

loss(train_X[:, :, :, 1:100], train_Y[:, 1:100])

accuracy(train_X[:,:,:,1:2000], train_Y[:, 1:2000])
accuracy(valid_X, valid_Y)

### unfreeze, train all parameters
### probably not working right

orig_params = deepcopy(freezed_params);
orig_trainable = deepcopy(trainable_params)

x1a = model(train_X[:, :, :, 1:10])

lr_range = 10 .^ (collect(range(-7, stop=-2, step=0.2) ))
best_lr, lr_losses, lr_plot = lr_finder(freezed_params, pets_dl, Adam(0.001), lr_range; reset_method=orig_params)
lr_plot
best_lr

loss(train_X[:, :, :, 1:100], train_Y[:, 1:100])

accuracy(train_X[:,:,:, 1:2000], train_Y[:, 1:2000])
accuracy(valid_X, valid_Y)

reset_params!(freezed_params, orig_params);

x1b = model(train_X[:, :, :, 1:10])

## not exactly the same...
hcat(x1a[:,2], x1b[:,2])

all([all(freezed_params[i] .== orig_params[i]) for i in 1:length(orig_params)])
all([all(trainable_params[i] .== orig_trainable[i]) for i in 1:length(orig_trainable)])

loss(train_X[:, :, :, 1:100], train_Y[:, 1:100])

accuracy(train_X[:,:,:, 1:2000], train_Y[:, 1:2000])
accuracy(valid_X, valid_Y)

opt = ADAM(10^(best_lr - 0.5)) |> gpu

reset_params!(freezed_params, orig_params);

for epoch = 1:epochs
    println("Epoch: $epoch")
    ##    Flux.train!(loss, Flux.params(model), pets_dl, opt, cb = evalcb)

    Flux.train!(loss, freezed_params, pets_dl, opt, cb = evalcb)
end

loss(train_X[:, :, :, 1:100], train_Y[:, 1:100])

accuracy(train_X[:,:,:, 1:2000], train_Y[:, 1:2000])
accuracy(valid_X, valid_Y)

all([trainable_params[i] == orig_trainable[i] for i in 1:length(trainable_params)])
all([freezed_params[i] == orig_params[i] for i in 1:length(freezed_params)])

### not resetting correctly ... could be some bug in reset_params! function, probably parameters are not lined up...
### or Flux.train! is altering some stuff in the model used for predicting
new_params = deepcopy(freezed_params)

reset_params!(freezed_params, orig_params);

loss(train_X[:, :, :, 1:100], train_Y[:, 1:100])

accuracy(train_X[:,:,:, 1:2000], train_Y[:, 1:2000])
accuracy(valid_X, valid_Y)

reset_params!(freezed_params, new_params);

loss(train_X[:, :, :, 1:100], train_Y[:, 1:100])

accuracy(train_X[:,:,:, 1:2000], train_Y[:, 1:2000])
accuracy(valid_X, valid_Y)

[maximum(abs.(new_params[i].-orig_params[i])) for i in 1:length(orig_params)]
[maximum(abs.(trainable_params[i].-orig_trainable[i])) for i in 1:length(orig_trainable)]

```

Try FastAI.jl

```{julia}

using FastAI

## make sure start from a fresh model

model = Chain(
    Metalhead.ResNet(18; pretrain=true).layers[1],  # Use all layers of ResNet except the last dense layer
    Chain(
      AdaptiveMeanPool((1, 1)),
        MLUtils.flatten,
        Dense(512 => n_breeds),
        softmax
    )
) |> gpu

pets_dl = Flux.DataLoader((train_X, train_Y), batchsize=10, shuffle=true) |> gpu

# Set up loss function, optimizer, callbacks, and learner:
lossfn = Flux.Losses.logitcrossentropy

lossfn = loss

## lossfn = Flux.Losses.crossentropy
optimizer = Flux.Adam()
callbacks = [ToGPU(), Metrics(accuracy)]
learner = Learner(
    model, pets_dl,
    optimizer, lossfn,
    callbacks...
)

# Train for 5 epochs:
@info "Training"
finetune!(learner, 2, base_lr=0.003)

```

Julia uses lexical scoping...

```{julia}

function f1(x)
    x + 1
end

ff = function(x)
    f1(x)
end

ff2 = function(f1)
    (x) -> f1(x)
end

ff2 = ff2(f1)

ff(1)
ff2(1)

function f1(x)
    x + 10
end

ff(1)
ff2(1)

```

